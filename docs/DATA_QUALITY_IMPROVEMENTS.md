# Data Quality Improvements for Congressional Trade Ingestion

## ðŸŽ¯ Overview

This document outlines the comprehensive data quality improvements implemented for the congressional trade ingestion system. These improvements address ticker extraction, security enrichment, trade backfill, and enhanced reporting capabilities.

## ðŸ“‹ Table of Contents

1. [System Architecture](#system-architecture)
2. [Core Components](#core-components)
3. [Data Quality Metrics](#data-quality-metrics)
4. [Ticker Extraction](#ticker-extraction)
5. [Security Enrichment](#security-enrichment)
6. [Trade Backfill](#trade-backfill)
7. [Manual Review System](#manual-review-system)
8. [Usage Examples](#usage-examples)
9. [Testing](#testing)
10. [Performance Considerations](#performance-considerations)

---

## ðŸ—ï¸ System Architecture

The improved system consists of several interconnected components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Congressional Data Ingestion                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   CSV Reader    â”‚  â”‚ Ticker Extractor â”‚  â”‚ Security Enricherâ”‚  â”‚
â”‚  â”‚   - Pandas      â”‚  â”‚  - Regex         â”‚  â”‚  - Exchange     â”‚  â”‚
â”‚  â”‚   - CSV Module  â”‚  â”‚  - Fuzzy Match   â”‚  â”‚  - Asset Type   â”‚  â”‚
â”‚  â”‚   - Validation  â”‚  â”‚  - Normalization â”‚  â”‚  - Sector       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                     â”‚                     â”‚          â”‚
â”‚           â–¼                     â–¼                     â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Trade Parser   â”‚  â”‚  Trade Creator  â”‚  â”‚ Trade Backfillerâ”‚  â”‚
â”‚  â”‚  - Date Parse   â”‚  â”‚  - Validation   â”‚  â”‚  - Linking      â”‚  â”‚
â”‚  â”‚  - Amount Parse â”‚  â”‚  - Batch Insert â”‚  â”‚  - Reconciling  â”‚  â”‚
â”‚  â”‚  - Type Parse   â”‚  â”‚  - Error Handle â”‚  â”‚  - Reporting    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚                              â”‚
â”‚                                  â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Quality Metrics â”‚  â”‚ Manual Review   â”‚  â”‚   Reporting     â”‚  â”‚
â”‚  â”‚  - Counters     â”‚  â”‚  - CSV Export   â”‚  â”‚  - Statistics   â”‚  â”‚
â”‚  â”‚  - Patterns     â”‚  â”‚  - Suggestions  â”‚  â”‚  - Summaries    â”‚  â”‚
â”‚  â”‚  - Confidence   â”‚  â”‚  - Prioritize   â”‚  â”‚  - Insights     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”§ Core Components

### 1. CongressionalDataIngester

The main orchestrator class that coordinates all data ingestion activities.

**Key Features:**
- Batch processing with configurable batch sizes
- Comprehensive error handling and recovery
- Progress tracking and logging
- Configurable processing options

**Configuration Options:**
```python
ingester = CongressionalDataIngester(session)
ingester.auto_create_securities = True      # Auto-create missing securities
ingester.auto_enrich_securities = True      # Auto-enrich with metadata
ingester.skip_invalid_trades = True         # Skip invalid data
ingester.batch_size = 1000                  # Batch commit size
```

### 2. TickerExtractor

Advanced ticker extraction with multiple fallback strategies.

**Features:**
- Pattern-based extraction from asset descriptions
- Fuzzy matching against company names
- Ticker normalization and validation
- Confidence scoring for extracted tickers

### 3. SecurityEnricher

Enriches securities with missing metadata.

**Features:**
- Exchange determination based on ticker patterns
- Asset type classification
- Sector assignment based on company names
- Default data setup for exchanges, sectors, and asset types

### 4. TradeBackfiller

Links trades to securities after enrichment.

**Features:**
- Processes trades with missing security_id
- Multiple matching strategies (ticker, name, description)
- Batch processing for performance
- Statistics tracking

---

## ðŸ“Š Data Quality Metrics

### Comprehensive Tracking

The system tracks extensive metrics for data quality assessment:

```python
@dataclass
class DataQualityMetrics:
    # Import counts
    total_rows_processed: int = 0
    csv_files_processed: int = 0
    years_processed: int = 0
    
    # Member counts
    total_members: int = 0
    members_created: int = 0
    
    # Trade counts
    total_trades: int = 0
    trades_with_security: int = 0
    trades_without_security: int = 0
    
    # Ticker extraction
    tickers_extracted: int = 0
    tickers_from_ticker_field: int = 0
    tickers_from_asset_description: int = 0
    tickers_from_fuzzy_matching: int = 0
    tickers_failed_extraction: int = 0
    
    # Security enrichment
    securities_enriched: int = 0
    trades_backfilled: int = 0
    
    # Error tracking
    failed_trades: int = 0
    parsing_errors: int = 0
    
    # Collections for detailed analysis
    unmatched_tickers: Counter = field(default_factory=Counter)
    unmatched_assets: Counter = field(default_factory=Counter)
    common_parsing_errors: Counter = field(default_factory=Counter)
```

### Performance Metrics

- **Processing Time**: Total time spent on ingestion
- **Average Time per Row**: Performance per record
- **Success Rates**: Percentage of successful operations
- **Confidence Scores**: Quality assessment of extracted data

---

## ðŸŽ¯ Ticker Extraction

### Multiple Extraction Strategies

The system employs a hierarchical approach to ticker extraction:

#### 1. Primary Strategy: Ticker Field
- **Confidence**: 0.9
- **Source**: Direct ticker field value
- **Validation**: Format and exclusion list checking

#### 2. Pattern-Based Extraction
- **Confidence**: 0.8
- **Source**: Regex patterns on asset descriptions
- **Patterns**:
  - `\(([A-Z]{1,5})\)` - Ticker in parentheses
  - `-\s*([A-Z]{1,5})(?:\s|$)` - Ticker after dash
  - `([A-Z]{1,5})$` - Ticker at end of string
  - `([A-Z]{1,5})\s+Class\s+[A-Z]` - Ticker with class designation

#### 3. Fuzzy Company Name Matching
- **Confidence**: 0.7
- **Source**: Company name lookup table
- **Features**:
  - Exact company name matching
  - Partial keyword matching
  - Extensive company-to-ticker mapping

#### 4. Mixed Content Extraction
- **Confidence**: 0.6
- **Source**: Pattern matching in mixed content
- **Patterns**:
  - `([A-Z]{1,5})\s+[PS](?:\s|$)` - Ticker with transaction type
  - Word-by-word validation

### Ticker Normalization

```python
def _normalize_ticker(self, ticker: str) -> str:
    """Normalize ticker symbol."""
    ticker = ticker.strip().upper()
    ticker = ticker.replace('.', '-')  # BRK.A -> BRK-A
    
    # Apply custom replacements
    if ticker in self.ticker_replacements:
        ticker = self.ticker_replacements[ticker]
    
    return ticker
```

### Company Name Mapping

Extensive mapping of company names to tickers:

```python
company_ticker_mapping = {
    'APPLE INC': 'AAPL',
    'MICROSOFT CORPORATION': 'MSFT',
    'AMAZON.COM INC': 'AMZN',
    'ALPHABET INC': 'GOOGL',
    'TESLA INC': 'TSLA',
    # ... 100+ mappings
}
```

---

## ðŸ¢ Security Enrichment

### Automatic Classification

The system automatically enriches securities with:

#### Exchange Determination
- **Cash/Money Market**: CASH exchange
- **Foreign Tickers**: OTC exchange (ending with 'F')
- **ETF Patterns**: NYSE for major ETFs
- **Default**: NYSE for most US stocks

#### Asset Type Classification
- **Cash/Money Market**: CASH asset type
- **Bonds**: BOND asset type (Treasury, Municipal)
- **ETFs**: ETF asset type
- **REITs**: REIT asset type
- **ADRs**: ADR asset type
- **Default**: STK (Common Stock)

#### Sector Assignment
- **Technology**: Microsoft, Apple, Google, Amazon, etc.
- **Healthcare**: Pfizer, Johnson & Johnson, Merck, etc.
- **Financial Services**: JPMorgan, Goldman Sachs, Bank of America, etc.
- **Consumer Discretionary**: Disney, Nike, Starbucks, etc.
- **Energy**: Exxon, Chevron, ConocoPhillips, etc.

### Default Data Setup

The system ensures required reference data exists:

```python
# Default Exchanges
default_exchanges = [
    {'code': 'NYSE', 'name': 'New York Stock Exchange'},
    {'code': 'NASDAQ', 'name': 'NASDAQ Global Select Market'},
    {'code': 'AMEX', 'name': 'American Stock Exchange'},
    {'code': 'OTC', 'name': 'Over-the-Counter'},
    {'code': 'CASH', 'name': 'Cash/Money Market'},
]

# Default Asset Types
default_asset_types = [
    {'code': 'STK', 'name': 'Common Stock', 'risk_level': 3},
    {'code': 'ETF', 'name': 'Exchange Traded Fund', 'risk_level': 2},
    {'code': 'BOND', 'name': 'Bond', 'risk_level': 1},
    {'code': 'CASH', 'name': 'Cash/Money Market', 'risk_level': 1},
]
```

---

## ðŸ”„ Trade Backfill

### Linking Strategy

The backfill process attempts to link trades to securities using:

1. **Ticker Matching**: Direct ticker lookup
2. **Asset Name Matching**: Fuzzy matching on asset names
3. **Description Parsing**: Re-extract tickers from descriptions

### Process Flow

```python
def backfill_trades(self) -> Dict[str, int]:
    # Get trades without security_id
    unlinked_trades = session.query(CongressionalTrade).filter(
        CongressionalTrade.security_id.is_(None)
    ).all()
    
    for trade in unlinked_trades:
        security = self._find_security_for_trade(trade)
        if security:
            trade.security_id = security.id
            stats['trades_linked'] += 1
    
    session.commit()
    return stats
```

### Performance Optimization

- **Batch Processing**: Process trades in batches
- **Caching**: Cache security lookups
- **Indexing**: Leverages database indexes for fast lookups

---

## ðŸ“ Manual Review System

### Automated Item Generation

The system generates manual review items for:

#### Unmatched Tickers
```python
ManualReviewItem(
    category='ticker',
    description=f'Unmatched ticker: {ticker}',
    raw_data={'ticker': ticker, 'count': count},
    suggested_fix=f'Review and add mapping for {ticker}'
)
```

#### Unmatched Assets
```python
ManualReviewItem(
    category='asset',
    description=f'Unmatched asset: {asset}',
    raw_data={'asset': asset, 'count': count},
    suggested_fix=f'Review and add mapping for {asset}'
)
```

#### Missing Security Information
```python
ManualReviewItem(
    category='security',
    description=f'{count} securities missing exchange information',
    raw_data={'count': count},
    suggested_fix='Add exchange information for securities'
)
```

### CSV Export

Manual review items are exported to CSV for easy review:

```csv
Category,Description,Raw Data,Suggested Fix,Confidence
ticker,Unmatched ticker: XYZ,"{""ticker"": ""XYZ"", ""count"": 5}",Review and add mapping for XYZ,0.0
asset,Unmatched asset: Unknown Company,"{""asset"": ""Unknown Company"", ""count"": 2}",Review and add mapping for Unknown Company,0.0
```

---

## ðŸ“‹ Usage Examples

### Basic Ingestion

```python
from domains.congressional.ingestion import CongressionalDataIngester

# Initialize with database session
with get_db_session() as session:
    ingester = CongressionalDataIngester(session)
    
    # Run ingestion
    results = ingester.import_congressional_data_from_csvs_sync('/path/to/csvs')
    
    # Review results
    print(f"Processed {results['total_rows_processed']} rows")
    print(f"Created {results['total_trades']} trades")
    print(f"Ticker extraction rate: {results['tickers_extracted'] / results['total_rows_processed'] * 100:.1f}%")
```

### Advanced Configuration

```python
# Configure ingester
ingester = CongressionalDataIngester(session)
ingester.auto_create_securities = True
ingester.auto_enrich_securities = True
ingester.skip_invalid_trades = False  # Strict mode
ingester.batch_size = 500

# Run with custom settings
results = ingester.import_congressional_data_from_csvs_sync('/path/to/csvs')
```

### Individual Component Usage

```python
# Ticker extraction only
extractor = TickerExtractor()
ticker, source, confidence = extractor.extract_ticker('', 'Apple Inc. (AAPL)')
print(f"Extracted {ticker} from {source} with confidence {confidence}")

# Security enrichment only
enricher = SecurityEnricher(session)
security = Security(ticker='AAPL', name='Apple Inc.')
if enricher.enrich_security(security):
    print(f"Enriched {security.ticker} with exchange_id={security.exchange_id}")

# Trade backfill only
backfiller = TradeBackfiller(session)
stats = backfiller.backfill_trades()
print(f"Backfilled {stats['trades_linked']} trades")
```

---

## ðŸ§ª Testing

### Test Suite

A comprehensive test suite validates all components:

```bash
# Run all tests
python app/src/scripts/test_data_quality_improvements.py

# Individual component tests
python -c "from scripts.test_data_quality_improvements import test_ticker_extraction; test_ticker_extraction()"
```

### Test Coverage

- **Ticker Extraction**: 10 test cases covering all strategies
- **Security Enrichment**: 5 securities with different characteristics
- **Data Quality Metrics**: Metrics tracking validation
- **Full Pipeline**: End-to-end integration test
- **Manual Review**: Review item generation test

### Test Data

The test suite uses realistic data with common issues:

```python
test_cases = [
    # Good data
    ['Smith', '12345', 'C', 'Apple Inc.', 'AAPL', 'P', '01/15/2023', ...],
    
    # Ticker extraction challenges
    ['Brown', '12347', 'JT', 'Alphabet Inc. Class C (GOOGL)', '', 'P', ...],
    
    # Fuzzy matching needed
    ['Taylor', '12350', 'C', 'Walt Disney Company', '', 'P', ...],
    
    # Problematic data
    ['White', '12360', 'C', 'Some Unknown Company XYZ', 'XYZ99', 'P', ...],
]
```

---

## âš¡ Performance Considerations

### Optimization Strategies

#### Batch Processing
- **Configurable Batch Size**: Default 1000 records
- **Database Commits**: Batch commits for performance
- **Memory Management**: Process in chunks to avoid memory issues

#### Caching
- **Member Cache**: Cache congress members during ingestion
- **Security Lookups**: Cache security lookups
- **Pattern Matching**: Pre-compile regex patterns

#### Database Optimization
- **Bulk Inserts**: Use bulk insert operations
- **Index Usage**: Leverage database indexes
- **Connection Pooling**: Reuse database connections

### Performance Metrics

Typical performance on modern hardware:

- **Processing Rate**: 1000-2000 records/second
- **Memory Usage**: 200-500MB for large datasets
- **Database Operations**: 10-50 operations/second
- **Ticker Extraction**: 5000-10000 extractions/second

### Scaling Considerations

For large datasets (>100k records):

1. **Increase Batch Size**: Use batch_size=5000
2. **Parallel Processing**: Process multiple CSV files in parallel
3. **Database Tuning**: Optimize database configuration
4. **Memory Scaling**: Increase available memory

---

## ðŸ” Monitoring and Alerting

### Key Metrics to Monitor

1. **Success Rates**
   - Ticker extraction success rate (target: >80%)
   - Trade-to-security linking rate (target: >90%)
   - Overall processing success rate (target: >95%)

2. **Data Quality**
   - Number of unmatched tickers
   - Number of unmatched assets
   - Number of parsing errors

3. **Performance**
   - Processing time per batch
   - Memory usage
   - Database query performance

### Alerting Thresholds

```python
# Example monitoring configuration
ALERT_THRESHOLDS = {
    'ticker_extraction_rate': 0.8,      # Alert if <80%
    'trade_linking_rate': 0.9,          # Alert if <90%
    'processing_success_rate': 0.95,    # Alert if <95%
    'max_processing_time': 1800,        # Alert if >30 minutes
    'max_memory_usage': 1024,           # Alert if >1GB
}
```

---

## ðŸ“š Best Practices

### Data Preparation

1. **CSV Validation**: Ensure consistent CSV format
2. **Encoding**: Use UTF-8 encoding for all files
3. **Date Formats**: Standardize date formats where possible
4. **Asset Naming**: Use consistent company naming conventions

### Error Handling

1. **Graceful Degradation**: Continue processing when possible
2. **Detailed Logging**: Log all issues with context
3. **Recovery Strategies**: Implement retry logic
4. **Data Validation**: Validate data before processing

### Maintenance

1. **Regular Updates**: Update company-to-ticker mappings
2. **Pattern Refinement**: Improve regex patterns based on new data
3. **Performance Monitoring**: Track performance trends
4. **Data Quality Review**: Regular review of manual review items

---

## ðŸ”® Future Enhancements

### Planned Improvements

1. **Machine Learning Integration**
   - ML-based ticker extraction
   - Automated pattern learning
   - Confidence score refinement

2. **Real-time Processing**
   - Stream processing capabilities
   - Real-time data validation
   - Immediate error notification

3. **Advanced Analytics**
   - Data quality dashboards
   - Trend analysis
   - Predictive quality metrics

4. **External Data Integration**
   - Third-party data validation
   - Market data enrichment
   - Regulatory filing integration

### API Enhancements

1. **REST API**: Expose ingestion functionality via REST API
2. **WebSocket**: Real-time progress updates
3. **GraphQL**: Flexible data querying
4. **Batch API**: Bulk operations support

---

## ðŸ“ž Support and Troubleshooting

### Common Issues

#### Low Ticker Extraction Rate
- **Symptoms**: <50% ticker extraction success
- **Causes**: Poor data quality, missing mappings
- **Solutions**: Update company mappings, improve patterns

#### Memory Issues
- **Symptoms**: Out of memory errors
- **Causes**: Large datasets, insufficient memory
- **Solutions**: Reduce batch size, increase memory

#### Performance Issues
- **Symptoms**: Slow processing, timeouts
- **Causes**: Database bottlenecks, large files
- **Solutions**: Optimize queries, parallel processing

### Debugging Tips

1. **Enable Debug Logging**: Set log level to DEBUG
2. **Use Test Data**: Start with small test datasets
3. **Monitor Resources**: Track CPU, memory, and disk usage
4. **Review Logs**: Check logs for error patterns

### Contact Information

For technical support:
- **Email**: support@capitolscope.com
- **Documentation**: https://docs.capitolscope.com
- **Issues**: https://github.com/capitolscope/issues

---

## ðŸ“„ Changelog

### Version 2.0.0 (Current)
- âœ… Enhanced ticker extraction with multiple strategies
- âœ… Comprehensive security enrichment
- âœ… Automated trade backfill
- âœ… Manual review system
- âœ… Advanced data quality metrics
- âœ… Performance optimizations

### Version 1.0.0 (Legacy)
- Basic CSV ingestion
- Simple ticker extraction
- Manual security creation
- Limited error handling

---

*This documentation is maintained by the CapitolScope development team. Last updated: December 2024*